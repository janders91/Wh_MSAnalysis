{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cQ1UjUv0zci"
   },
   "source": [
    "## Particle collisions\n",
    "\n",
    "\n",
    "Author: [Alessio Devoto](https://alessiodevoto.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oN1Qezxr599j",
    "outputId": "b0f21fd4-1e2a-4184-8cb1-5c419078034f"
   },
   "outputs": [],
   "source": [
    "# Here we just install it from pip\n",
    "# If you want access to the files, you can still download the repository though\n",
    "!pip install torchmetrics --quiet     # for accuracy and metrics\n",
    "!pip install sparticles --quiet   --use-deprecated=legacy-resolver     # our library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cC7zWAeH4o4z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import glob\n",
    "from sparticles.transforms import MakeHomogeneous\n",
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I4mNYrRQd7-r"
   },
   "outputs": [],
   "source": [
    "# Random state for shuffling the dataset.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Names of the directories in the raw directory.\n",
    "RAW_DIR_NAMES = ['signal', 'singletop', 'ttbar']\n",
    "\n",
    "# Constant labels for noise and signal.\n",
    "SIGNAL_LABEL = 1\n",
    "BACKGROUND_LABEL = 0\n",
    "\n",
    "# Match between directory and event type.\n",
    "EVENT_LABELS = {\n",
    "    'signal': SIGNAL_LABEL,\n",
    "    'singletop': BACKGROUND_LABEL,\n",
    "    'ttbar': BACKGROUND_LABEL\n",
    "}\n",
    "\n",
    "# Number of events to keep for each event type.\n",
    "# The total number of events in the dataset is the sum of the values in this dictionary.\n",
    "# We can use these values to have a more balanced dataset.\n",
    "DEFAULT_EVENT_SUBSETS = {\n",
    "    'signal': 463056,\n",
    "    'singletop': 242614,\n",
    "    'ttbar': 6093298\n",
    "}\n",
    "\n",
    "# These are the columns we should keep from the raw pandas dataframe.\n",
    "# The nan columns are just a hack as we need to have the same number of columns for each row.\n",
    "USEFUL_COLS = [\n",
    "    # jet 1\n",
    "    'pTj1', 'etaj1', 'phij1', 'j1_quantile', 'nan', 'nan',\n",
    "    # jet 2\n",
    "    'pTj2', 'etaj2', 'phij2', 'j2_quantile', 'nan', 'nan',\n",
    "    # jet 3\n",
    "    'pTj3', 'etaj3', 'phij3', 'j3_quantile', 'nan', 'nan',\n",
    "    # b1\n",
    "    'pTb1', 'etab1', 'phib1', 'b1_quantile', 'b1m', 'nan',\n",
    "    # b2\n",
    "    'pTb2', 'etab2', 'phib2', 'b2_quantile', 'b2m', 'nan',\n",
    "    # lepton\n",
    "    'pTl1', 'etal1', 'phil1', 'nan', 'nan', 'nan',\n",
    "    # energy\n",
    "    'ETMiss', 'nan', 'ETMissPhi', 'nan', 'nan', 'metsig_New',\n",
    "]\n",
    "\n",
    "# A markdown table to display the structure of a single event.\n",
    "EVENT_TABLE = \"\"\"\n",
    "    Each event is a graph with 6/7 nodes. Each node is built from the raw file as follows:\n",
    "\n",
    "    | Particle          | Feature 1 | Feature 2 | Feature 3   | Feature 4     | Feature 5 | Feature 6    |\n",
    "    |-------------------|-----------|-----------|-------------|---------------|-----------|--------------|\n",
    "    | jet1              |  'pTj1'   | 'etaj1'   |   'phij1'   | 'j1_quantile' |    nan    |     nan      |\n",
    "    | jet2              |  'pTj2'   | 'etaj2'   |   'phij2'   | 'j2_quantile' |    nan    |     nan      |\n",
    "    | jet3 (optional)   |  'pTj3'   | 'etaj3'   |   'phij3'   | 'j3_quantile' |    nan    |     nan      |\n",
    "    | b1                |  'pTb1'   | 'etab1'   |   'phib1'   | 'b1_quantile' |   'b1m'   |     nan      |\n",
    "    | b2                |  'pTb2'   | 'etab2'   |   'phib2'   | 'b2_quantile' |   'b2m'   |     nan      |\n",
    "    | lepton            |  'pTl1'   | 'etal1'   |   'phil1'   |      nan      |    nan    |     nan      |\n",
    "    | energy            | 'ETMiss'  |   nan     | 'ETMissPhi' |      nan      |    nan    | 'metsig_New' |\n",
    "    \"\"\"\n",
    "\n",
    "class EventsDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset of graphs representing collisions of particles.\n",
    "    There are three types of event:\n",
    "        - signal, label 1\n",
    "        - singletop, label 0\n",
    "        - ttbar, label 0\n",
    "\n",
    "    Each event is a graph with 6 or 7 nodes and 6 attributes. Graphs are fully connected.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        url (str): URL to download the dataset from.\n",
    "        event_subsets (dict, optional): Dictionary containing the number of events to keep for each event type. Defaults to {'signal': 463056, 'singletop': 242614, 'ttbar': 6093298}.\n",
    "        add_edge_index (bool, optional): Whether to add the fully connected edge index to the data objects. Defaults to True.\n",
    "        delete_raw_archive (bool, optional): Whether to delete the raw archive after extracting it. Defaults to False.\n",
    "        transform (callable, optional): A function/transform that takes in a `torch_geometric.data.Data` object and returns a transformed version. The data object will be transformed before every access. Defaults to None.\n",
    "        pre_transform (callable, optional): A function/transform that takes in a `torch_geometric.data.Data` object and returns a transformed version. The data object will be transformed before being saved to disk. Defaults to None.\n",
    "        pre_filter (callable, optional): A function that takes in a `torch_geometric.data.Data` object and returns a boolean value, indicating whether the data object should be included in the final dataset. Defaults to None.\n",
    "        download_type: If it is set to 1, it extracts all the h5 files in signal folder, if it is set to 2, it extracts the h5 file with all mixed signals.\n",
    "        signal_filename: The name of the signal file to be processed.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            root,\n",
    "            url,\n",
    "            event_subsets: dict = DEFAULT_EVENT_SUBSETS,\n",
    "            add_edge_index: bool = True,\n",
    "            delete_raw_archive: bool = False,\n",
    "            transform=None,\n",
    "            pre_transform=None,\n",
    "            pre_filter=None,\n",
    "            download_type: int = 2,\n",
    "            signal_filename: str = 'Wh_hbb_fullMix.h5'):  # Added signal_filename argument\n",
    "\n",
    "        self.url = url\n",
    "        self.delete_raw_archive = delete_raw_archive\n",
    "        self.event_subsets = event_subsets\n",
    "        self.add_edge_index = add_edge_index\n",
    "        self.download_type = download_type  # Store download type\n",
    "        self.signal_filename = signal_filename  # Store signal filename\n",
    "        self.subset_string = '_'.join([f'{k}_{v}' for k, v in sorted(self.event_subsets.items())])\n",
    "\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return RAW_DIR_NAMES\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # Notice the processed file names depend on the number of events we keep for each event type.\n",
    "        return [f'events_{self.subset_string}.pt']\n",
    "\n",
    "    @property\n",
    "    def event_structure(self):\n",
    "        \"\"\"\n",
    "        Returns the event structure of the dataset.\n",
    "        The event structure is a table that describes the different types of events that can occur in the dataset.\n",
    "        Returns:\n",
    "            str: A string containing a markdown table representing the event structure of the dataset.\n",
    "        \"\"\"\n",
    "        return EVENT_TABLE\n",
    "\n",
    "    def download(self):\n",
    "        # Download raw directories to `self.raw_dir`.\n",
    "        print(f'Downloading {self.url} to {self.raw_dir}...')\n",
    "        print('This may take a while...')\n",
    "        raw_archive = download_url(self.url, self.raw_dir, filename='events.tar', log=False)\n",
    "\n",
    "        print('Extracting files...')\n",
    "        with tarfile.open(raw_archive) as tar:\n",
    "            if self.download_type == 1:\n",
    "                # Extract all files in the folder\n",
    "                tar.extractall(self.raw_dir)\n",
    "            elif self.download_type == 2:\n",
    "                members = tar.getmembers()\n",
    "                for member in members:\n",
    "                    # Extract the file which contains all signals mixed or the specified signal file.\n",
    "                    if 'signal' in member.name and self.signal_filename not in member.name:\n",
    "                        continue\n",
    "                    tar.extract(member, self.raw_dir)\n",
    "\n",
    "        if self.delete_raw_archive:\n",
    "            os.remove(raw_archive)\n",
    "\n",
    "        # In case the compressed file contains a single directory, we move the files to the raw_dir.\n",
    "        print('Moving files...')\n",
    "        for dir in self.raw_file_names:\n",
    "            dirpath = glob.glob(f'{self.raw_dir}/**/{dir}', recursive=True)[0]\n",
    "            shutil.move(dirpath, self.raw_dir)\n",
    "            print(f'Moved {dirpath} to {self.raw_dir}')\n",
    "\n",
    "        print('Cleaning up...')\n",
    "        # Remove the directories which are not in self.raw_file_names.\n",
    "        for f in os.listdir(self.raw_dir):\n",
    "            if f not in self.raw_file_names + ['events.tar']:\n",
    "                try:\n",
    "                    shutil.rmtree(os.path.join(self.raw_dir, f))\n",
    "                except NotADirectoryError:\n",
    "                    os.remove(os.path.join(self.raw_dir, f))\n",
    "\n",
    "        \"\"\"\n",
    "        At this stage, we should have the following directory structure.\n",
    "        Notice h5 file names can change.\n",
    "\n",
    "        root\n",
    "        ├── processed\n",
    "        └── raw\n",
    "            ├── signal\n",
    "            │   └── <specified signal file>\n",
    "            ├── singletop\n",
    "            │   └── singletop.h5\n",
    "            └── ttbar\n",
    "                └── ttbar.h5\n",
    "        \"\"\"\n",
    "\n",
    "    def process(self):\n",
    "     # Create a dictionary of h5 files, where keys are the event types and values are the path to the h5 file.\n",
    "     # We don't know the .h5 file names, so we use glob to find them.\n",
    "\n",
    "     h5_files = {}\n",
    "\n",
    "     for d in self.raw_file_names:\n",
    "        dir_path = os.path.join(self.raw_dir, d)\n",
    "        if d == 'signal':\n",
    "            signal_file_path = os.path.join(dir_path, self.signal_filename)\n",
    "            if os.path.exists(signal_file_path):\n",
    "                h5_files[d] = signal_file_path\n",
    "        else:\n",
    "            h5_files[d] = glob.glob(f'{dir_path}/*.h5', recursive=True)[0]\n",
    "\n",
    "     data_list = []\n",
    "\n",
    "     for event_type, h5_file in h5_files.items():\n",
    "        # Labels is the same for all events in the same directory.\n",
    "        label = EVENT_LABELS[event_type]\n",
    "        # Read data into pandas dataframe and filter out useless columns.\n",
    "        graphs = pd.read_hdf(h5_file)\n",
    "        graphs.drop(columns=list(set(graphs.columns) - set(USEFUL_COLS)), inplace=True)\n",
    "        # Hackish way to have all rows with the same number of columns.\n",
    "        graphs['nan'] = torch.nan\n",
    "        # Rearrange columns to have the same order as USEFUL_COLS and create index column.\n",
    "        graphs = graphs[USEFUL_COLS].reset_index()\n",
    "        # Shuffle the dataframe and possibly keep only part of it.\n",
    "        graphs = graphs.sample(n=self.event_subsets[event_type], random_state=RANDOM_STATE)\n",
    "\n",
    "        for row in tqdm(graphs.values, total=graphs.shape[0], desc=f'Processing events in {h5_file}'):\n",
    "            event_id = int(row[0])\n",
    "            graph_features = row[1:]\n",
    "            if (event_type not in 'ttbar' and event_type  not in  'singletop' and event_type  not in 'signal'):\n",
    "                values_array = []\n",
    "\n",
    "                # Iterate through every element in the array\n",
    "                for element in row[1:]:\n",
    "                    # If the element is a dictionary add all of the values to a new array\n",
    "                    if isinstance(element, dict):\n",
    "                        values_array.extend(element.values())\n",
    "                    # If the element is a NaN simply add a simple NaN to the array\n",
    "                    elif np.isnan(element):\n",
    "                        values_array.append(np.nan)\n",
    "\n",
    "                # Convert the array of values into a numpy array\n",
    "                graph_features = np.array(values_array)\n",
    "\n",
    "            x = torch.from_numpy(graph_features).reshape(7, -1)\n",
    "\n",
    "            x = x[x[:, 0] > 0]\n",
    "\n",
    "            # graphs are all fully connected\n",
    "            edge_index = None\n",
    "            if self.add_edge_index:\n",
    "                directed_edge_index = torch.combinations(torch.arange(x.shape[0]), 2)\n",
    "                edge_index = torch.cat([directed_edge_index, directed_edge_index.flip(1)], dim=0).T\n",
    "\n",
    "            # TODO should we add the edge index here? Knowing it is fully connected, does it make sense to waste space for this ?\n",
    "            # TODO make the event id a constant across multiple datasets\n",
    "\n",
    "            data_list.append(Data(\n",
    "                x=x,\n",
    "                event_id=f'{event_type}_{event_id}',\n",
    "                y=label,\n",
    "                edge_index=edge_index,\n",
    "            ))\n",
    "\n",
    "     if self.pre_filter is not None:\n",
    "        data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "     if self.pre_transform is not None:\n",
    "        data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "     data, slices = self.collate(data_list)\n",
    "     torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfsRKG482JGV"
   },
   "source": [
    "### 3. Training a Simple GNN\n",
    "\n",
    "Here you can define your GNN, train them on the data, and see what happens. You can do it with [pytorch-lightning](https://lightning.ai/) or in plain PyTorch. Here I do it in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JkmBsFl64_uv",
    "outputId": "ad42d39b-3cc5-4ebb-9039-f8464f1ef740",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Processing events in /hepstore/janders/Analysis_Wh/raw/signal/Wh_hbb_fullMix.h5:   0%| | 0/2000 [00:00<?, ?i\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got numpy.ndarray)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEventsDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/hepstore/janders/Analysis_Wh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://cernbox.cern.ch/s/0nh0g7VubM4ndoh/download\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelete_raw_archive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_edge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_subsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msingletop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mttbar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMakeHomogeneous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 104\u001b[0m, in \u001b[0;36mEventsDataset.__init__\u001b[0;34m(self, root, url, event_subsets, add_edge_index, delete_raw_archive, transform, pre_transform, pre_filter, download_type, signal_filename)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignal_filename \u001b[38;5;241m=\u001b[39m signal_filename  \u001b[38;5;66;03m# Store signal filename\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubset_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_subsets\u001b[38;5;241m.\u001b[39mitems())])\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/ATLAS/MastersStudent/Wh_MSAnalysis/conda/envs/Analysis_Wh/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ATLAS/MastersStudent/Wh_MSAnalysis/conda/envs/Analysis_Wh/lib/python3.10/site-packages/torch_geometric/data/dataset.py:115\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ATLAS/MastersStudent/Wh_MSAnalysis/conda/envs/Analysis_Wh/lib/python3.10/site-packages/torch_geometric/data/dataset.py:262\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    261\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_transform.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    265\u001b[0m fs\u001b[38;5;241m.\u001b[39mtorch_save(_repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform), path)\n",
      "Cell \u001b[0;32mIn[3], line 227\u001b[0m, in \u001b[0;36mEventsDataset.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Convert the array of values into a numpy array\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     graph_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values_array)\n\u001b[0;32m--> 227\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m x[x[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# graphs are all fully connected\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got numpy.ndarray)"
     ]
    }
   ],
   "source": [
    "dataset = EventsDataset(\n",
    "    root='/hepstore/janders/Analysis_Wh',\n",
    "    url='https://cernbox.cern.ch/s/0nh0g7VubM4ndoh/download',\n",
    "    delete_raw_archive=False,\n",
    "    add_edge_index=True,\n",
    "    event_subsets={'signal': 2000, 'singletop': 1000, 'ttbar': 1000},\n",
    "    transform=MakeHomogeneous(),\n",
    "    download_type=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzkXWEjt_R4N"
   },
   "source": [
    "Before training, we have to split the data into train and test set as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xulrWE1V82zM",
    "outputId": "a9b65836-d6e0-4419-cafe-f3d0e35e2772"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# generate indices: instead of the actual data we pass in integers\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    train_size=0.8,\n",
    "    stratify=[g.y.item() for g in dataset], # to have balanced subsets\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_graphs = Subset(dataset, train_indices)\n",
    "test_graphs = Subset(dataset, test_indices)\n",
    "\n",
    "print(f'Train set contains {len(train_graphs)} graphs, Test set contains {len(test_graphs)} graphs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpEdpggOBjk9"
   },
   "outputs": [],
   "source": [
    "# Dataloaders allow us to batch datasets together and speed up the training\n",
    "# For more info about dataloaders: https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#mini-batches\n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=96, shuffle=True)\n",
    "test_loader = DataLoader(test_graphs, batch_size=96, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTJcKvkA_rp9"
   },
   "source": [
    "We define a simple GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV0mDj7D3okK"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch\n",
    "\n",
    "MANUAL_SEED = 1234\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(MANUAL_SEED)\n",
    "        self.conv1 = GCNConv(input_channels, hidden_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.aggregate = global_mean_pool\n",
    "        self.head = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.aggregate(x, batch)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UORlA_qK5Vy3"
   },
   "source": [
    "We run the training, now saving the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2esBM1fMQCo9",
    "outputId": "8475feeb-069f-4be9-e6f9-ffb02cea0f6b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm # for nice bar\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(input_channels=12, hidden_channels=36, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "compute_acc = Accuracy(task='binary')\n",
    "\n",
    "train_epochs = 50  # Training epochs\n",
    "\n",
    "model.train()\n",
    "for epoch in range(train_epochs):\n",
    "  print(f'Training epoch: {epoch}')\n",
    "  for batch in tqdm(train_loader, leave=False):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "    loss = F.binary_cross_entropy_with_logits(out.squeeze(), batch.y.float())\n",
    "    loss.backward()\n",
    "    train_losses.append(loss.detach().item())\n",
    "    train_accuracies.append(compute_acc(out.detach().squeeze(), batch.y.float()))\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f'Validation epoch: {epoch}')\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, leave=False):\n",
    "      out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "      loss = F.binary_cross_entropy_with_logits(out.squeeze(), batch.y.float())\n",
    "      test_losses.append(loss.detach().item())\n",
    "      test_accuracies.append(compute_acc(out.detach().squeeze(), batch.y.float()))\n",
    "\n",
    "  # NEW: Save model snapshot for every epoch... you can save it every 10?\n",
    "  torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }, f'epoch_{epoch}_snapshot.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofVopUKR5Vy3"
   },
   "source": [
    "Plot the training and test loss and acuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "id": "Q-jC04iwIHAu",
    "outputId": "d632cab9-f59e-4db1-adfa-9315fd9fa396"
   },
   "outputs": [],
   "source": [
    "#NEW\n",
    "import torch\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved snapshots\n",
    "snapshot_paths = [\"epoch_{}_snapshot.pth\".format(epoch) for epoch in range(train_epochs)]\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "total_epochs = 0\n",
    "\n",
    "for snapshot_path in snapshot_paths:\n",
    "    snapshot = torch.load(snapshot_path)\n",
    "    train_losses.extend(snapshot['train_losses'])\n",
    "    test_losses.extend(snapshot['test_losses'])\n",
    "    train_accuracies.extend(snapshot['train_accuracies'])\n",
    "    test_accuracies.extend(snapshot['test_accuracies'])\n",
    "\n",
    "epochs = np.arange(1, train_epochs + 1)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"Train loss\", \"Test loss\", \"Train acc\", \"Test acc\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=epochs[:len(train_losses)], y=train_losses), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs[:len(test_losses)], y=test_losses), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=epochs[:len(train_accuracies)], y=train_accuracies), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs[:len(test_accuracies)], y=test_accuracies), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, width=1000, title='Training results', showlegend=False)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVfELMdD5Vy3"
   },
   "source": [
    "## 4. Plot output score\n",
    "\n",
    "Plot the score directly from the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "QlhDFJ70f9RF",
    "outputId": "a3d2b966-02bd-4490-b133-927bc174f529"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming test_loader.dataset.y contains the true labels and out.detach().squeeze() contains the predicted logits\n",
    "\n",
    "# Concatenate true labels and predicted logits\n",
    "true_labels = torch.cat([batch.y for batch in test_loader.dataset]).numpy()\n",
    "predicted_logits = torch.cat([model(batch.x.float(), batch.edge_index, batch.batch).detach().squeeze() for batch in test_loader]).numpy()\n",
    "\n",
    "# Convert logits to probabilities using sigmoid function\n",
    "predicted_probs = torch.sigmoid(torch.tensor(predicted_logits)).numpy()\n",
    "\n",
    "# Filter predictions based on true labels (signal and background)\n",
    "signal_indices = true_labels == 1\n",
    "background_indices = true_labels == 0\n",
    "\n",
    "signal_probs = predicted_probs[signal_indices]\n",
    "background_probs = predicted_probs[background_indices]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(signal_probs, bins=50, color='skyblue', alpha=0.7, label='Signal', density=True)\n",
    "plt.hist(background_probs, bins=50, color='orange', alpha=0.7, label='Background', density=True)\n",
    "\n",
    "plt.title('Model Output Distribution on Validation Dataset')\n",
    "plt.xlabel('Predicted Probabilities')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnRFSb9_5Vy3"
   },
   "source": [
    "## 5. Load data from snapsot\n",
    "\n",
    "You should take an epoch after the loss and acuracy have leveled off but before any overtraining.  Here I just take the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmsxiFEnUly6"
   },
   "outputs": [],
   "source": [
    "def add_score_from_snapshot(dataset, model, snap_name):\n",
    "    # Load snapshot and set model to that point\n",
    "    checkpoint = torch.load(snap_name)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    # Loop over dataset and apply model\n",
    "    new_data = []\n",
    "    for elem in dataset:\n",
    "        output = model(elem.x.float(), elem.edge_index, elem.batch)\n",
    "        prob = torch.sigmoid(output).squeeze().detach()\n",
    "        # Make a new variable on the graph to add the score\n",
    "        elem.score = prob\n",
    "        new_data.append(elem)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHc2v31e5Vy3"
   },
   "source": [
    "Load snapshot and add score to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylH05lHqVdBR",
    "outputId": "74dbdf31-331f-42ea-a53c-9e8bcbc9a840"
   },
   "outputs": [],
   "source": [
    "graphs_with_score = add_score_from_snapshot(test_graphs, model, \"epoch_40_snapshot.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcMhHnXD5Vy4"
   },
   "source": [
    "Plot score from snapshot.  Should be same as above for last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "dOcMI-op5Vy4",
    "outputId": "6e41ca8e-d20f-4f36-e375-c7bcb48315ba"
   },
   "outputs": [],
   "source": [
    "# Plot score from the snapshot\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"GNN Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.hist([d.score for d in graphs_with_score if d.y == 0], alpha = 0.5, bins = 50, density = True, label = \"Background\", color = \"orange\")\n",
    "plt.hist([d.score for d in graphs_with_score if d.y == 1], alpha = 0.5, bins = 50, density = True, label = \"Signal\", color = \"skyblue\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qh8mKa_n5Vy4"
   },
   "source": [
    "Plot a varibale (here lepton pT) for high and low score background, compared to signal.  The high-score background has a longer tail, more like the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "vzCXHmrJ5Vy4",
    "outputId": "ecd45e61-0171-4249-a835-2af94060d84c"
   },
   "outputs": [],
   "source": [
    "# Get leptons for signal events\n",
    "sig_lep= [g.x[5] if g.x.shape == (7,6) else g.x[4] for g in graphs_with_score if g.y == 1]\n",
    "\n",
    "# Get leptons for background events with low or high score\n",
    "threshold = 0.5\n",
    "bkg_lep_high = [g.x[5] if g.x.shape == (7,6) else g.x[4] for g in graphs_with_score if g.y == 0 and g.score > threshold]\n",
    "bkg_lep_low = [g.x[5] if g.x.shape == (7,6) else g.x[4] for g in graphs_with_score if g.y == 0 and g.score < threshold]\n",
    "\n",
    "# Plot\n",
    "bins = np.linspace(0,1000, 100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel(\"$p_T^{lep}$\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist([f[0].item() for f in sig_lep], bins = bins,\n",
    "        color=\"skyblue\", alpha = 0.5, label = \"Signal\", density = True)\n",
    "plt.hist([f[0].item() for f in bkg_lep_high], bins = bins,\n",
    "        color=\"orange\", alpha = 0.5, label = f\"Bkg (score > {threshold}\", density = True)\n",
    "plt.hist([f[0].item() for f in bkg_lep_low], bins = bins,\n",
    "        color=\"green\", alpha = 0.5, label =  f\"Bkg (score < {threshold}\", density = True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quBi1g5-5Vy4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
